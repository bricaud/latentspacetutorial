{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Image Embeddings Using Hugging Face Models\n",
    "\n",
    "This tutorial explores how to visualize the latent spaces of image embeddings using Vision Transformer models from the Hugging Face Transformers library. We'll cover what latent spaces are in the context of images, why they're important, and how to extract and visualize them from pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Image Latent Spaces\n",
    "\n",
    "A **latent space** (also known as a latent representation or embedding space) is a compressed representation of data where similar items are positioned closer together. These spaces are \"latent\" because they represent hidden factors that explain the observed data.\n",
    "\n",
    "### Why are image latent spaces important?\n",
    "\n",
    "- They enable dimensionality reduction, converting high-dimensional image data into more manageable lower-dimensional representations\n",
    "- They capture visual similarities and semantic relationships between images\n",
    "- They allow for meaningful manipulations (style transfer, image editing, etc.)\n",
    "- They provide insights into how vision models internally represent visual information\n",
    "\n",
    "In computer vision, visualizing latent spaces can help us understand:\n",
    "- How models cluster visually similar images\n",
    "- Which visual features the model considers important\n",
    "- Potential biases in the model's representations\n",
    "- How different vision architectures learn different representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "Let's install the necessary libraries for our tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# See notebook on text embeddings for more info on how to install the packages\n",
    "#\n",
    "# %pip install transformers torch scikit-learn matplotlib pandas seaborn umap-learn Pillow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as offsetbox  # Import offsetbox module for thumbnail visualization\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Embeddings from Vision Transformers\n",
    "\n",
    "Let's explore the latent space of a vision model. We'll use a pre-trained Vision Transformer (ViT) to generate embeddings for a set of images, and then visualize how these embeddings are organized in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Vision Transformer\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "vit_model = ViTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing images\n",
    "\n",
    "For this tutorial, we'll use the CIFAR-10 dataset which contains images from 10 different classes. If you have your own set of images, you could use those instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for processing local images\n",
    "'''\n",
    "# If you have local image files\n",
    "image_paths = [\n",
    "    \"path/to/cat1.jpg\", \"path/to/cat2.jpg\", \"path/to/cat3.jpg\",\n",
    "    \"path/to/dog1.jpg\", \"path/to/dog2.jpg\", \"path/to/dog3.jpg\",\n",
    "    # etc.\n",
    "]\n",
    "image_labels = [\"cat\", \"cat\", \"cat\", \"dog\", \"dog\", \"dog\", ...]\n",
    "\n",
    "images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small subset of images from the CIFAR-10 dataset\n",
    "dataset = load_dataset(\"cifar10\", split=\"train[:100]\")  # Load 100 images for demonstration\n",
    "\n",
    "# Display some sample images from the dataset\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    img = dataset[i][\"img\"]\n",
    "    label = dataset.features[\"label\"].int2str(dataset[i][\"label\"])\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Class: {label}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract images and labels\n",
    "# Handle images properly - CIFAR-10 images are numpy arrays\n",
    "images = []\n",
    "image_labels = []\n",
    "for img in dataset:\n",
    "    # CIFAR-10 returns numpy arrays in the 'img' field\n",
    "    if isinstance(img[\"img\"], np.ndarray):\n",
    "        images.append(Image.fromarray(img[\"img\"]))\n",
    "    else:\n",
    "        # If it's already a PIL Image, use it directly\n",
    "        images.append(img[\"img\"])\n",
    "    image_labels.append(dataset.features[\"label\"].int2str(img[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Image Embeddings\n",
    "\n",
    "Now, let's use our Vision Transformer model to generate embeddings for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image embeddings\n",
    "def get_image_embeddings(images, model, feature_extractor):\n",
    "    # Initialize an empty list to store embeddings\n",
    "    embeddings = []\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Process each image\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        for image in images:\n",
    "            # Preprocess the image\n",
    "            inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Get the embedding (we'll use the [CLS] token embedding)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            embeddings.append(embedding[0])\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for our images\n",
    "image_embeddings = get_image_embeddings(images, vit_model, feature_extractor)\n",
    "\n",
    "print(f\"Generated embeddings with shape: {image_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Vision Transformer Embeddings\n",
    "\n",
    "The Vision Transformer (ViT) represents images by:\n",
    "\n",
    "1. Splitting the image into fixed-size patches (e.g., 16x16 pixels)\n",
    "2. Linearly embedding each patch\n",
    "3. Adding position embeddings\n",
    "4. Passing these embeddings through transformer encoder blocks\n",
    "\n",
    "The output embedding we extracted (the [CLS] token) serves as a global representation of the entire image, capturing both local features and their relationships. These embeddings are typically high-dimensional (768 dimensions for the base model we're using)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction for Visualization\n",
    "\n",
    "Let's apply dimensionality reduction techniques to visualize our image embeddings in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction techniques\n",
    "# PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "image_embeddings_pca = pca.fit_transform(image_embeddings)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "image_embeddings_tsne = tsne.fit_transform(image_embeddings)\n",
    "\n",
    "# UMAP\n",
    "umap_reducer = UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "image_embeddings_umap = umap_reducer.fit_transform(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to visualize image embeddings\n",
    "def plot_image_embeddings(embeddings, labels, title):\n",
    "    # Create a DataFrame for easier plotting\n",
    "    df = pd.DataFrame({\n",
    "        'x': embeddings[:, 0],\n",
    "        'y': embeddings[:, 1],\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "    color_map = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Create a scatter plot\n",
    "    for label in unique_labels:\n",
    "        label_data = df[df['label'] == label]\n",
    "        plt.scatter(label_data['x'], label_data['y'], \n",
    "                    c=[color_map[label]], label=label, alpha=0.7, s=100)\n",
    "    \n",
    "    # Add title and legend\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.legend(fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image embeddings with PCA\n",
    "plot_image_embeddings(image_embeddings_pca, image_labels, \n",
    "                     \"Image Embeddings Visualization using PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image embeddings with t-SNE\n",
    "plot_image_embeddings(image_embeddings_tsne, image_labels, \n",
    "                       \"Image Embeddings Visualization using t-SNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image embeddings with UMAP\n",
    "plot_image_embeddings(image_embeddings_umap, image_labels, \n",
    "                       \"Image Embeddings Visualization using UMAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Images in Latent Space\n",
    "\n",
    "Let's create a more informative visualization by showing actual thumbnail images at their embedding positions. This helps us to better understand what kinds of images are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_in_latent_space(embeddings, images, labels, title, sample_size=50):\n",
    "    # Sample a subset of images if we have too many\n",
    "    if len(images) > sample_size:\n",
    "        indices = np.random.choice(len(images), sample_size, replace=False)\n",
    "        sampled_embeddings = embeddings[indices]\n",
    "        sampled_images = [images[i] for i in indices]\n",
    "        sampled_labels = [labels[i] for i in indices]\n",
    "    else:\n",
    "        sampled_embeddings = embeddings\n",
    "        sampled_images = images\n",
    "        sampled_labels = labels\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(16, 14))\n",
    "    \n",
    "    # Plot each image at its embedding position\n",
    "    for i, (x, y, img, label) in enumerate(zip(sampled_embeddings[:, 0], \n",
    "                                              sampled_embeddings[:, 1], \n",
    "                                              sampled_images,\n",
    "                                              sampled_labels)):\n",
    "        # Convert PIL image to numpy array if it's not already\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_array = np.array(img)\n",
    "        else:\n",
    "            img_array = img\n",
    "            \n",
    "        # Create an OffsetImage of the image\n",
    "        img_box = offsetbox.OffsetImage(img_array, zoom=2)\n",
    "        ab = offsetbox.AnnotationBbox(img_box, (x, y), frameon=True, \n",
    "                                     pad=0.2, bboxprops=dict(edgecolor=plt.cm.tab10(hash(label) % 10)))\n",
    "        ax.add_artist(ab)\n",
    "    \n",
    "    # Add scatter points for legend (invisible, just for the legend)\n",
    "    unique_labels = sorted(set(sampled_labels))\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = np.array(sampled_labels) == label\n",
    "        ax.scatter(sampled_embeddings[mask, 0], sampled_embeddings[mask, 1], \n",
    "                   c=[plt.cm.tab10(i % 10)], label=label, alpha=0)\n",
    "    \n",
    "    # Set title and legend\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.legend(fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Remove ticks since the actual values aren't meaningful\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add a grid for visual reference\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes we have the original images as numpy arrays\n",
    "# Convert all our images to numpy arrays if they're PIL images\n",
    "image_arrays = [np.array(img) for img in images]\n",
    "\n",
    "# Plot images in t-SNE space (since t-SNE often gives the best visual clustering)\n",
    "plot_images_in_latent_space(image_embeddings_tsne, image_arrays, image_labels, \n",
    "                            \"Images in Latent Space (t-SNE)\", sample_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing Clusters and Relationships\n",
    "\n",
    "Let's analyze what kinds of relationships the model has learned by examining which images are grouped together in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize the average embedding for each class\n",
    "def plot_class_centroids(embeddings, labels, title):\n",
    "    # Create a DataFrame for the embedding data\n",
    "    df = pd.DataFrame({\n",
    "        'x': embeddings[:, 0],\n",
    "        'y': embeddings[:, 1],\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    # Calculate centroids for each class\n",
    "    centroids = df.groupby('label').mean().reset_index()\n",
    "    \n",
    "    # Plot the embeddings with class centroids\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "    color_map = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Plot individual points\n",
    "    for label in unique_labels:\n",
    "        label_data = df[df['label'] == label]\n",
    "        plt.scatter(label_data['x'], label_data['y'], \n",
    "                    c=[color_map[label]], label=label, alpha=0.3, s=50)\n",
    "    \n",
    "    # Plot centroids with labels\n",
    "    for i, row in centroids.iterrows():\n",
    "        plt.scatter(row['x'], row['y'], c=[color_map[row['label']]], \n",
    "                   s=300, edgecolors='black', linewidths=2, alpha=1.0)\n",
    "        plt.annotate(row['label'], (row['x'], row['y']), \n",
    "                     fontsize=14, fontweight='bold', ha='center', va='center')\n",
    "    \n",
    "    # Add title and legend\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.legend(fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "# Plot the class centroids\n",
    "class_centroids = plot_class_centroids(image_embeddings_tsne, image_labels, \n",
    "                                     \"Class Centroids in t-SNE Space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between centroids to understand class relationships\n",
    "def calculate_centroid_distances(centroids):\n",
    "    # Extract coordinates and labels\n",
    "    labels = centroids['label'].values\n",
    "    points = centroids[['x', 'y']].values\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    n = len(points)\n",
    "    distances = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distances[i, j] = np.sqrt(np.sum((points[i] - points[j])**2))\n",
    "    \n",
    "    # Create a DataFrame for the distance matrix\n",
    "    distance_df = pd.DataFrame(distances, index=labels, columns=labels)\n",
    "    \n",
    "    return distance_df\n",
    "\n",
    "# Calculate and display centroid distances\n",
    "centroid_distances = calculate_centroid_distances(class_centroids)\n",
    "print(\"Distances between class centroids:\")\n",
    "display(centroid_distances)\n",
    "\n",
    "# Visualize the distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(centroid_distances, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Distance Between Class Centroids in Latent Space', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction Techniques Compared\n",
    "\n",
    "Let's discuss the different dimensionality reduction techniques we've used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Principal Component Analysis (PCA)\n",
    "\n",
    "**Advantages:**\n",
    "- Linear and deterministic (same results on multiple runs)\n",
    "- Preserves global structure and directions of maximum variance\n",
    "- Computationally efficient, even for larger datasets\n",
    "- Can explain how much variance each dimension captures\n",
    "\n",
    "**Disadvantages:**\n",
    "- Cannot capture non-linear relationships in the data\n",
    "- May not preserve local structure well\n",
    "- Performance degrades in very high dimensions with non-linear manifolds\n",
    "\n",
    "**Best used when:**\n",
    "- You want to understand global variance directions\n",
    "- Data has approximately linear relationships\n",
    "- You need deterministic, reproducible results\n",
    "- You're working with very large datasets where computational efficiency matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "**Advantages:**\n",
    "- Excellent at preserving local structure and finding clusters\n",
    "- Can reveal patterns hidden in high-dimensional space\n",
    "- Handles non-linear relationships well\n",
    "- Works well for visualizing natural clusters in data\n",
    "\n",
    "**Disadvantages:**\n",
    "- Stochastic (different results on multiple runs)\n",
    "- Does not preserve global structure well\n",
    "- Can be computationally expensive\n",
    "- Sensitive to hyperparameters (especially perplexity)\n",
    "- Not good for downstream tasks beyond visualization\n",
    "\n",
    "**Best used when:**\n",
    "- You want to visualize clusters and local neighborhoods\n",
    "- Global distances between separated clusters are less important\n",
    "- You want to explore the data without strict reproducibility requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "**Advantages:**\n",
    "- Preserves both local and global structure better than t-SNE\n",
    "- Faster than t-SNE, especially for larger datasets\n",
    "- More stable across multiple runs than t-SNE\n",
    "- Can be used for dimensionality reduction as a preprocessing step, not just visualization\n",
    "- Supports supervised dimension reduction\n",
    "\n",
    "**Disadvantages:**\n",
    "- Still somewhat stochastic (though more stable than t-SNE)\n",
    "- Has multiple hyperparameters that need tuning\n",
    "- Theoretical foundations more complex than PCA or t-SNE\n",
    "\n",
    "**Best used when:**\n",
    "- You want a balance between preserving local and global structure\n",
    "- You need faster performance than t-SNE for larger datasets\n",
    "- You plan to use the reduced dimensions for downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Best Practices\n",
    "\n",
    "### Key takeaways for visualizing image embeddings in latent space:\n",
    "\n",
    "1. **Understand your goal:**\n",
    "   - For cluster analysis and pattern discovery, t-SNE or UMAP often work best\n",
    "   - For understanding variance directions, PCA is more appropriate\n",
    "   - For balancing local and global structure, UMAP is a good choice\n",
    "\n",
    "2. **Be aware of limitations:**\n",
    "   - Dimensionality reduction always loses information\n",
    "   - Different algorithms preserve different aspects of the data\n",
    "   - Random initializations can affect results (especially for t-SNE)\n",
    "\n",
    "3. **Tips for better visualizations:**\n",
    "   - Try multiple dimensionality reduction techniques and compare results\n",
    "   - Experiment with hyperparameters (perplexity for t-SNE, n_neighbors for UMAP)\n",
    "   - Use appropriate color coding and labels\n",
    "   - Consider using thumbnail images to directly see patterns\n",
    "   - For large datasets, consider subsampling or using incremental techniques\n",
    "\n",
    "4. **Interpreting image embedding visualizations:**\n",
    "   - Proximity in the visualization generally means visual similarity\n",
    "   - Clusters often represent visually similar objects or scenes\n",
    "   - Directions in the latent space may correspond to visual attributes (colors, shapes, textures)\n",
    "   - Outliers could be unusual images or potential errors in the dataset\n",
    "\n",
    "5. **Beyond visualization:**\n",
    "   - Image embeddings can be used for image retrieval, classification, and anomaly detection\n",
    "   - Distance metrics in latent space can quantify visual similarity\n",
    "   - Clustering in latent space can identify natural image groupings\n",
    "   - Latent space manipulations can enable image editing and style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Exploration Ideas\n",
    "\n",
    "Here are some additional ideas to explore image embeddings further:\n",
    "\n",
    "1. **Compare different models:** Try different vision architectures (ViT vs. ResNet vs. EfficientNet) and see how their latent spaces differ\n",
    "\n",
    "2. **Layer-wise analysis:** Extract embeddings from different layers of a model to see how representations evolve\n",
    "\n",
    "3. **Fine-tuning effects:** Compare latent spaces before and after fine-tuning on a specific task\n",
    "\n",
    "4. **Multimodal embeddings:** Explore joint text-image embedding spaces using models like CLIP\n",
    "\n",
    "5. **Image interpolation:** Interpolate between two embeddings and decode back to images to see the transition\n",
    "\n",
    "6. **Interactive visualization:** Use tools like Tensorboard Projector for interactive image embedding exploration\n",
    "\n",
    "7. **Attention visualization:** Combine latent space visualization with attention maps for deeper insights\n",
    "\n",
    "8. **Style transfer and image editing:** Use latent space manipulations to edit image features or transfer styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
